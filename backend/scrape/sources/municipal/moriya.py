# -*- coding: utf-8 -*-
"""
OMO Platform - å®ˆè°·å¸‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼

å®ˆè°·å¸‚å…¬å¼HPã‹ã‚‰ãŠçŸ¥ã‚‰ã›ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
"""

import hashlib
import sys
from pathlib import Path
from typing import List, Dict, Any
from urllib.parse import urljoin

# ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

from backend.scrape.core.base import MunicipalScraper
from backend.scrape.core.http import get_http_client
from backend.common.utils import compute_quick_hash, clean_image_urls


class MoriyaScraper(MunicipalScraper):
    """å®ˆè°·å¸‚å…¬å¼HPã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.http = get_http_client()
        
        # è¨­å®šã‹ã‚‰å–å¾—
        self.base_url = config.get("base_url", "https://www.city.moriya.ibaraki.jp")
        self.list_url = config.get("list_url", "https://www.city.moriya.ibaraki.jp/kurashi/oshirase/index.html")
        self.selectors = config.get("selectors", {})
        self.max_per_site = config.get("max_per_site", 10)
    
    def scrape(self) -> List[Dict[str, Any]]:
        """
        ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ
        
        Returns:
            ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã•ã‚ŒãŸè¨˜äº‹ã®ãƒªã‚¹ãƒˆ
        """
        if not self.is_enabled():
            print("â© å®ˆè°·å¸‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã¯ç„¡åŠ¹ã§ã™")
            return []
        
        print(f"\n--- å®ˆè°·å¸‚å…¬å¼HP ---")
        
        # ä¸€è¦§å–å¾—
        news_list = self.get_news_list()
        
        # è¨˜äº‹è©³ç´°ã‚’å–å¾—
        articles = []
        for info in news_list[:self.max_per_site]:
            try:
                article = self.get_article_detail(
                    url=info["url"],
                    list_title=info.get("list_title", ""),
                    published_date=info.get("date", "")
                )
                
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDç”Ÿæˆ
                doc_id = f"moriya_municipal_{hashlib.sha256(info['url'].encode()).hexdigest()}"
                article["doc_id"] = doc_id
                article["category"] = "moriya_municipal"
                article["original_url"] = info["url"]
                article["published_date_str"] = info.get("date", "")
                article["list_title"] = info.get("list_title", "")
                
                articles.append(article)
                
            except Exception as e:
                print(f"âŒ è¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼: {info['url']} | {e}")
        
        print(f"âœ… å®ˆè°·å¸‚: {len(articles)} ä»¶å–å¾—")
        return articles
    
    def get_news_list(self) -> List[Dict[str, Any]]:
        """
        ãŠçŸ¥ã‚‰ã›ä¸€è¦§ã‚’å–å¾—
        
        Returns:
            ãŠçŸ¥ã‚‰ã›æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        try:
            soup = self.http.get_soup(self.list_url, timeout=20)
            
            # ã‚»ãƒ¬ã‚¯ã‚¿ã§ä¸€è¦§ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—
            items = soup.select(self.selectors.get("list_item_container", "div.list_item"))
            
            results = []
            for item in items:
                # æ—¥ä»˜
                date_el = item.select_one(self.selectors.get("date", "span.date"))
                # ãƒªãƒ³ã‚¯
                link_el = item.select_one(self.selectors.get("link", "a"))
                
                if not (date_el and link_el):
                    continue
                
                date_text = date_el.get_text(strip=True)
                href = link_el.get("href")
                full_url = urljoin(self.base_url, href)
                link_text = link_el.get_text(strip=True)
                
                results.append({
                    "url": full_url,
                    "date": date_text,
                    "list_title": link_text
                })
            
            print(f"ğŸ“„ ä¸€è¦§å–å¾—: {len(results)} ä»¶")
            return results
            
        except Exception as e:
            print(f"âš ï¸ ä¸€è¦§å–å¾—å¤±æ•—: {e}")
            return []
    
    def get_article_detail(
        self,
        url: str,
        list_title: str,
        published_date: str
    ) -> Dict[str, Any]:
        """
        è¨˜äº‹è©³ç´°ã‚’å–å¾—
        
        Args:
            url: è¨˜äº‹URL
            list_title: ä¸€è¦§ãƒšãƒ¼ã‚¸ã§ã®ã‚¿ã‚¤ãƒˆãƒ«
            published_date: å…¬é–‹æ—¥æ–‡å­—åˆ—
        
        Returns:
            è¨˜äº‹è©³ç´°ãƒ‡ãƒ¼ã‚¿
        """
        soup = self.http.get_soup(url, timeout=20)
        
        # ã‚¿ã‚¤ãƒˆãƒ«
        title_el = soup.select_one(self.selectors.get("title", "h1.page_title"))
        page_title = title_el.get_text(strip=True) if title_el else (list_title or "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜")
        
        # æœ¬æ–‡
        content_body = soup.select_one(self.selectors.get("content_body", "div.main_content"))
        body_text = ""
        pdf_links = []
        image_links = []
        
        if content_body:
            # æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆ
            body_text = content_body.get_text(separator="\n", strip=True)
            body_lines = [ln.strip() for ln in body_text.splitlines()]
            body_text = "\n".join([ln for ln in body_lines if ln])
            
            # PDFãƒªãƒ³ã‚¯
            for a in content_body.select('a[href$=".pdf"]'):
                href = a.get("href")
                if href:
                    pdf_links.append(urljoin(self.base_url, href))
            
            # ç”»åƒãƒªãƒ³ã‚¯
            for img in content_body.select("img[src]"):
                src = img.get("src")
                if src:
                    image_links.append(urljoin(self.base_url, src))
        
        # ãƒã‚¤ã‚ºç”»åƒé™¤å»
        image_links = clean_image_urls(image_links)
        pdf_links = list(set(pdf_links))
        
        # è»½é‡ãƒãƒƒã‚·ãƒ¥è¨ˆç®—
        quick_hash = compute_quick_hash(
            body_text=body_text,
            pdf_links=pdf_links,
            image_links=image_links,
            page_title=page_title,
            published_date=published_date
        )
        
        return {
            "title": page_title,
            "body_text": body_text,
            "pdf_links": pdf_links,
            "image_links": image_links,
            "quick_hash": quick_hash,
        }
